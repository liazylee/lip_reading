{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T17:16:26.106932257Z",
     "start_time": "2024-04-11T17:16:25.852960387Z"
    }
   },
   "id": "2b1bcaebeeb1d772",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def mouth_extractor(file_path: str, scale_factor=1.3, min_neighbors=5, mouth_size=(140, 70)) -> None:\n",
    "    \"\"\"\n",
    "    Extract the mouth from the video and save as an npy file\n",
    "    :param file_path: Path to the video file\n",
    "    :param scale_factor: Parameter specifying how much the image size is reduced at each image scale\n",
    "    :param min_neighbors: Parameter specifying how many neighbors each candidate rectangle should have to retain it\n",
    "    :param mouth_size: Size of the extracted mouth region\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    base_path = file_path.split('.')[0]\n",
    "    # if not os.path.exists(base_path + '.npy'):\n",
    "    if file_path.endswith('.mpg'):\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        if not cap.isOpened():\n",
    "            raise Exception(\"Error: Could not open video.\")\n",
    "\n",
    "        frames = []\n",
    "        for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                raise Exception(\"Error: Could not read frame.\")\n",
    "            # 参数分别为低阈值和高阈值\n",
    "            faces = face_cascade.detectMultiScale(frame, scale_factor, min_neighbors)\n",
    "            if len(faces) == 0:\n",
    "                continue\n",
    "            for (x, y, w, h) in faces:\n",
    "                mouth_roi = frame[y + int(h / 2):y + h, x:x + w, ]\n",
    "                mouth_roi = cv2.resize(mouth_roi, mouth_size)\n",
    "                mouth_roi = cv2.cvtColor(mouth_roi, cv2.COLOR_BGR2GRAY)\n",
    "                frames.append(mouth_roi)\n",
    "\n",
    "        cap.release()\n",
    "        # Normalize frames\n",
    "        frames = np.array(frames, dtype=np.float32)\n",
    "        mean = np.mean(frames)\n",
    "        std = np.std(frames)\n",
    "        # Normalize frames\n",
    "        frames = (frames - mean) / std  # Broadcasting subtraction and division\n",
    "        # Save as npy file\n",
    "        np.save(base_path + '.npy', frames)\n",
    "        return frames\n",
    "    else:\n",
    "        raise Exception(\"Error: File format not supported.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T15:33:34.621573147Z",
     "start_time": "2024-04-10T15:33:34.198487567Z"
    }
   },
   "id": "3b6f83f2a1addfa2",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f9cf13fbe304924",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frames_tensor = mouth_extractor('/Users/zhenyili/research project/src/lipNet/data/s1/bbbmzn.mpg')\n",
    "print(frames_tensor.shape)\n",
    "# show the first 10 frames the np shape is (75, 1, 70, 140)\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n",
    "frames_tensor = np.clip(frames_tensor, 0, 1)\n",
    "# print(frames_tensor[0])\n",
    "for i in range(4):\n",
    "    ax[i].imshow(frames_tensor[i])\n",
    "    # ax[i].axis('off')  # no axes for this plot\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T15:32:04.929955707Z",
     "start_time": "2024-04-10T15:32:04.899925701Z"
    }
   },
   "id": "103daa1f06cf0529",
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frames_tensor = mouth_extractor('/Users/zhenyili/research project/src/lipNet/data/s1/bbbmzn.mpg')\n",
    "print(frames_tensor.shape)\n",
    "# show the first 10 frames the np shape is (75, 1, 70, 140)\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n",
    "frames_tensor = np.clip(frames_tensor, 0, 1)\n",
    "# print(frames_tensor[0])\n",
    "for i in range(4):\n",
    "    ax[i].imshow(frames_tensor[i])\n",
    "    # ax[i].axis('off')  # no axes for this plot\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T21:02:46.655908660Z",
     "start_time": "2024-04-01T21:02:46.611242901Z"
    }
   },
   "id": "87e7df09cb7c6c91",
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# from utils import mouth_extractor\n",
    "\n",
    "frames_tensor = mouth_extractor('/home/liazylee/jobs/python/AI/lip_reading/src/lipNet/data/s1/bbbmzn.mpg')\n",
    "# show the first 10 frames\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(frames_tensor[i + 30])\n",
    "    # ax[i].axis('off')  # no axes for this plot\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T15:33:24.678718434Z",
     "start_time": "2024-04-10T15:33:24.589089775Z"
    }
   },
   "id": "d5166fa27a5777d5",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.load('/Users/zhenyili/research project/src/lipNet/data/s1/.npy').shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T11:05:06.584063659Z",
     "start_time": "2024-03-17T11:05:06.047625105Z"
    }
   },
   "id": "2e1d67dfe064a18f",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.load('/home/liazylee/jobs/python/AI/lip_reading/src/lipNet/data/s1/bbbmzn.npy').shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T15:50:55.757917851Z",
     "start_time": "2024-04-10T15:50:55.716257990Z"
    }
   },
   "id": "4e19db0f6dd79387",
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frame = np.load('/home/liazylee/jobs/python/AI/lip_reading/src/lipNet/data/s1/bbbmzn.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T17:16:32.531884947Z",
     "start_time": "2024-04-11T17:16:32.526596047Z"
    }
   },
   "id": "cdacefd973d05ef",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show the first 10 frames\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(frame[i + 30], )\n",
    "    ax[i].axis('off')  # no axes for this plot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T17:16:54.605313290Z",
     "start_time": "2024-04-11T17:16:54.060218067Z"
    }
   },
   "id": "95662d0de097835f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_data(dir) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    data = []\n",
    "    # 递归搜索该目录下的文件含有align的文件\n",
    "    alignments = glob.glob(dir + '/**/*.align', recursive=True)\n",
    "    alignments_dict = {}\n",
    "    for align in alignments:\n",
    "        alignments_dict[align.split('/')[-1].split('.')[0]] = load_alignments(align)\n",
    "    for root, dir, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                video_frames = np.load(os.path.join(root, file))\n",
    "                video_frames = torch.from_numpy(video_frames).float()\n",
    "                filename = file.split('.')[0]\n",
    "                if filename in alignments_dict:\n",
    "                    alignments = alignments_dict[filename]\n",
    "                else:\n",
    "                    logging.warning(f'No alignment found for {filename}')\n",
    "                    alignments = ''\n",
    "                data.append((video_frames, alignments))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_alignments(path: str) -> torch.Tensor:\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        tokens = ''\n",
    "        for line in lines:\n",
    "            line = line.split()\n",
    "            if line[2] != 'sil':\n",
    "                tokens = tokens + ' ' + line[2]\n",
    "        tokens_np = np.array([ord(c) for c in tokens])\n",
    "        tokens_tensor = torch.from_numpy(tokens_np).float()\n",
    "        return tokens_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:21:34.807269Z",
     "start_time": "2024-03-12T18:21:33.824978Z"
    }
   },
   "id": "8900afa89088e2f0",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "def generate_alignments_text(dir: str) -> set[str]:\n",
    "    data = []\n",
    "    # 递归搜索该目录下的文件含有align的文件\n",
    "    alignments = glob.glob(dir + '/**/*.align', recursive=True)\n",
    "    for align in alignments:\n",
    "        with open(align, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.split()\n",
    "                if line[2] != 'sil':\n",
    "                    data.append(line[2])\n",
    "    data = set(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_text() -> set[str]:\n",
    "    data = generate_alignments_text(dir='/home/liazylee/jobs/python/AI/lip_reading/src/lipNet/data/')\n",
    "    # save to a file\n",
    "    print(len(data))\n",
    "    with open('text.txt', 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s \" % item)\n",
    "\n",
    "\n",
    "generate_text()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T20:44:43.211281771Z",
     "start_time": "2024-03-18T20:44:43.047543289Z"
    }
   },
   "id": "8dfe474c23d19e59",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = load_data('/home/liazylee/jobs/python/AI/lip_reading/src/lipNet/data/')\n",
    "print(data[0][0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:31:12.378560Z",
     "start_time": "2024-03-07T10:31:12.292429Z"
    }
   },
   "id": "8a3370823c099588",
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(data[0][1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:06:02.644888270Z",
     "start_time": "2024-03-06T21:06:02.642327607Z"
    }
   },
   "id": "ee6e92f3dc7cf495",
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pretain(dir: str) -> List:\n",
    "    \"\"\"\n",
    "    find all the video and extra the mouth region \n",
    "    :param dir: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    video_list = glob.glob(dir + '/**/*.mpg', recursive=True)\n",
    "    return video_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:21:40.723007Z",
     "start_time": "2024-03-12T18:21:40.720740Z"
    }
   },
   "id": "3f53886447a37cc9",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "video_list = pretain('/home/liazylee/jobs/python/AI/lip_reading/src/lipNet/data')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:22:59.860157Z",
     "start_time": "2024-03-07T10:22:59.825273Z"
    }
   },
   "id": "6cedea005ca60362",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "video_list = pretain('/Users/zhenyili/research project/src/lipNet/data')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:21:45.105921Z",
     "start_time": "2024-03-12T18:21:45.099608Z"
    }
   },
   "id": "f184155f5228a67",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(video_list[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:21:48.542541Z",
     "start_time": "2024-03-12T18:21:48.540115Z"
    }
   },
   "id": "ef8d2a12ad8ce13a",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "for i in range(10):\n",
    "    mouth_extractor(video_list[i])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:21:54.277766Z",
     "start_time": "2024-03-12T18:21:52.396433Z"
    }
   },
   "id": "1e5d25aaded5585f",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from config import DIR\n",
    "from dataset import LRNetDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Subset\n",
    "from typing import Any, Tuple\n",
    "\n",
    "\n",
    "def load_train_test_data() -> list[Subset[Any]]:\n",
    "    video_dataset = LRNetDataset(DIR)\n",
    "    train_size = int(0.8 * len(video_dataset))\n",
    "    test_size = len(video_dataset) - train_size\n",
    "    return torch.utils.data.random_split(video_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "def collate_fn(batch: list[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    inputs_tensors, targets_tensors = zip(*batch)\n",
    "    padded_inputs = pad_sequence(inputs_tensors, batch_first=True, padding_value=0)\n",
    "    padded_inputs = padded_inputs.permute(0, 4, 1, 2, 3)\n",
    "    padded_targets = pad_sequence(targets_tensors, batch_first=True, padding_value=0)\n",
    "    return padded_inputs, padded_targets,\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:22:04.146617Z",
     "start_time": "2024-03-12T18:22:04.141978Z"
    }
   },
   "id": "8c02f6ffab8d85c5",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from dataset_loader import LRNetDataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset, val_dataset = load_train_test_data()\n",
    "train_loader = LRNetDataLoader(train_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:23:35.050769Z",
     "start_time": "2024-03-12T18:23:34.909474Z"
    }
   },
   "id": "c9f77d8e3e8db1c7",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sample_inputs, *args = next(iter(train_loader))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:23:48.535633Z",
     "start_time": "2024-03-12T18:23:39.282429Z"
    }
   },
   "id": "1b768134c3e98b6d",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sample_inputs, _ = next(iter(train_loader))\n",
    "print(sample_inputs.shape)\n",
    "print(_.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:24:10.370694Z",
     "start_time": "2024-03-12T18:24:01.090403Z"
    }
   },
   "id": "120b7207e6a837d2",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_shape = sample_inputs.shape\n",
    "\n",
    "print(input_shape)\n",
    "print(list(map(lambda x: x.shape, args)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:24:21.554495Z",
     "start_time": "2024-03-12T18:24:21.552062Z"
    }
   },
   "id": "da9e5ac9f4f8372d",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_channels = input_shape[1]\n",
    "print(input_channels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:50:51.751102Z",
     "start_time": "2024-03-07T10:50:51.748901Z"
    }
   },
   "id": "22854db517275e14",
   "execution_count": 104,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from model import LRModel\n",
    "from torch import nn, optim\n",
    "\n",
    "model = LRModel().to(device)\n",
    "criterion = nn.CTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_loss_curve, val_loss_curve, train_wer_curve, val_wer_curve = [], [], [], []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:50:52.453561Z",
     "start_time": "2024-03-07T10:50:52.438051Z"
    }
   },
   "id": "88e8759b1cc487b0",
   "execution_count": 105,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_wer = 0\n",
    "    val_wer = 0\n",
    "    print(f'Epoch {epoch + 1}/{10}')\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # #[32, 75, 1, 70, 140]->[32,1,75,70,140]\n",
    "        # inputs = torch.permute(inputs, (0, 2, 1, 3, 4))\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        input_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(targets.size(0),), fill_value=targets.size(1), dtype=torch.long)\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{10}, Step {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_curve.append(train_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-07T10:55:43.636470Z"
    }
   },
   "id": "be39a6ebc3a17a38",
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import calculate_cer, calculate_wer\n",
    "\n",
    "# Example text\n",
    "reference = \"If you have specific test results or details you'd like translated please provide them and I can assist you further.\"\n",
    "hypothesis = \"If you have specific test results or details you'd like translated please provide them, and I can assist you further\"\n",
    "\n",
    "# Calculate CER\n",
    "cer = calculate_cer(reference, hypothesis)\n",
    "\n",
    "# Calculate WER\n",
    "wer = calculate_wer(reference, hypothesis)\n",
    "\n",
    "print(type(cer))\n",
    "print(type(wer))\n",
    "print(cer)\n",
    "print(wer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T09:34:39.289325558Z",
     "start_time": "2024-03-18T09:34:38.958105074Z"
    }
   },
   "id": "71024fad418e2a6c",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "# Your predicted and true strings\n",
    "predicted = ['', '', '', '', '', '', '', '']\n",
    "true = ['bin red with n three again', 'bin green with b seven again', 'set green with x four please',\n",
    "        'place red in c two now', 'set green at p zero now', 'place green with y nine soon',\n",
    "        'lay white at r eight please', 'bin red at s seven soon']\n",
    "\n",
    "# Calculate Word Error Rate (WER)\n",
    "word_error_rate = wer(true, predicted)\n",
    "print(f\"WER: {word_error_rate}\")\n",
    "\n",
    "# Calculate Character Error Rate (CER)\n",
    "character_error_rate = cer(true, predicted)\n",
    "print(f\"CER: {character_error_rate}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T20:27:50.056455409Z",
     "start_time": "2024-03-27T20:27:49.904428700Z"
    }
   },
   "id": "d8756e2d9d1474c1",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "text_outputs = [' lae ree it  eie pason', ' lat wree it g eie aon', ' lan wree it g eire ason',\n",
    "                ' lan wree it g ftie aon', ' lan ree it p eire ason', ' lae wree it g seie aon',\n",
    "                ' lat wree it e eie ason', ' lat ree it e tie aon']\n",
    "text_targets = [' set blue by k five again', ' bin white at t five again', ' lay blue at a three soon',\n",
    "                ' bin blue with x one again', ' place white by t seven again', ' set green in r six now',\n",
    "                ' lay green with c eight now', ' lay blue at a two now']\n",
    "\n",
    "wers = [wer(target, output) for target, output in zip(text_targets, text_outputs)]\n",
    "cers = [cer(target, output) for target, output in zip(text_targets, text_outputs)]\n",
    "print(wers)\n",
    "print(cers)\n",
    "average_wer = sum(wers) / len(wers)\n",
    "average_cer = sum(cers) / len(cers)\n",
    "print(average_wer)\n",
    "print(average_cer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:01:34.985214964Z",
     "start_time": "2024-03-14T09:01:34.667795848Z"
    }
   },
   "id": "ce76c2930bea158f",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from word_beam_search import WordBeamSearch\n",
    "\n",
    "corpus = 'a ba'  # two words \"a\" and \"ba\", separated by whitespace\n",
    "chars = 'ab '  # the characters that can be recognized (in this order)\n",
    "word_chars = 'ab'  # characters that form words\n",
    "\n",
    "# RNN output\n",
    "# 3 time-steps and 4 characters per time time (\"a\", \"b\", \" \", CTC-blank)\n",
    "mat = np.array([[[0.9, 0.1, 0.0, 0.0]],\n",
    "                [[0.0, 0.0, 0.0, 1.0]],\n",
    "                [[0.6, 0.4, 0.0, 0.0]]])\n",
    "\n",
    "# initialize word beam search (only do this once in your code)\n",
    "wbs = WordBeamSearch(25, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'), word_chars.encode('utf8'))\n",
    "\n",
    "# compute label string\n",
    "label_str = wbs.compute(mat)\n",
    "print(label_str)\n",
    "char_str = []  # decoded texts for batch\n",
    "for curr_label_str in label_str:\n",
    "    s = ''.join([chars[label] for label in curr_label_str])\n",
    "    char_str.append(s)\n",
    "print(char_str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T09:40:43.044988515Z",
     "start_time": "2024-03-18T09:40:42.623906001Z"
    }
   },
   "id": "6094883db23b47aa",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T15:47:10.112938735Z",
     "start_time": "2024-03-14T15:47:09.679617776Z"
    }
   },
   "id": "af58d7856e2662f5",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "\n",
    "def decode_sequences(y_pred, number_dict, beam_search=False, beam_width=10):\n",
    "    \"\"\"\n",
    "    根据模型输出解码序列，可以选择使用贪心解码或束搜索解码。\n",
    "    \n",
    "    :param y_pred: 模型输出的logits，形状为(batch_size, timesteps, num_classes)。\n",
    "    :param number_dict: 包含所有类别的字典，键为类别索引，值为对应字符。\n",
    "    :param beam_search: 是否使用束搜索解码。默认为False，即使用贪心解码。\n",
    "    :param beam_width: 束搜索的宽度。默认为10。\n",
    "    :return: 解码后的序列列表。\n",
    "    \"\"\"\n",
    "    blank_id = 27  # 假设空白标签的索引为27\n",
    "\n",
    "    if beam_search:\n",
    "        # 初始化CTCBeamDecoder\n",
    "        decoder = CTCBeamDecoder([number_dict.get(i, '') for i in range(max(number_dict.keys()) + 1)],\n",
    "                                 beam_width=beam_width, blank_id=blank_id, log_probs_input=True)\n",
    "        # 使用CTCBeamDecoder进行解码\n",
    "        beam_result, _, _, out_lens = decoder.decode(y_pred.permute(1, 0, 2))\n",
    "        # 处理解码结果\n",
    "        decoded_sequences = []\n",
    "        for i in range(beam_result.size(0)):\n",
    "            seq = beam_result[i, 0, :out_lens[i, 0]]  # 选取得分最高的序列\n",
    "            decoded_sequences.append(''.join([number_dict.get(x, '') for x in seq]))\n",
    "    else:\n",
    "        # 使用贪心解码\n",
    "        decoded_sequences = []\n",
    "        _, max_indices = torch.max(y_pred, dim=-1)\n",
    "        for path in max_indices:\n",
    "            decoded_sequence = [number_dict.get(path[0].item(), '')]\n",
    "            for i in range(1, len(path)):\n",
    "                if path[i] != path[i - 1] and path[i].item() != blank_id:\n",
    "                    decoded_sequence.append(number_dict.get(path[i].item(), ''))\n",
    "            decoded_sequences.append(''.join(decoded_sequence))\n",
    "    return decoded_sequences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7afb7eab57fa8f82",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
